\mychapter{7}{Discussion}
\section{Observation based on results}

Our first step was to determine what dimensionality reduction algorithms to use.
We decided to use Laplacian eigmaps \cite{belkin2003laplacian} and Diffusion Maps \cite{coifman2006} which are both non-linear dimensionality reduction algorithms.
why?----to be addressed later.
Initially, we smoothed the spike data using an exponential kernel defined by

\[
  K_{\tau}(t) =
  \begin{cases}
                 \frac{1}{\tau} \exp(-\frac{t}{\tau}) & \text{if $t > 0$} \\
                  0 & \text{elsewhere} 
  \end{cases}
\]

\subsection{Observations}
We found that using Laplacian eigen maps to get an embedding based on the firing rate tried to
recovered the position of the rat but could not reflect any variations along the path
e.g the animal could have looked away from the track or run in a ragged fashion around the track.\\

We also found that whenever there were gaps between the receptive fields (cases with no spikes),Laplacian Eigen Maps (LAM) performed poorly.
This is because the nearest neighbor graph (based on the Euclidean distance used in (LAM) yields several connected components (i.e, the graph is disconnected).
Since the eigen value decomposition step  in LAM is only applied on the largest connected
component of the graph, the eigen vectors output by LAM are shorter than the total number of original data points (spike times). Thus the embedding provided by LAM is in accurate in some
instances due to the "coverage" problem.\\

Diffusion Maps (DM) algorithm tends to run out of memory in case of large instances
so we were unable to compare the performance of both LAM and DM when using the firing rate.
This is not true: First redo the analysis on sonet since you need to show that
previous/next time is an improvement over the traditional firing rate.

\subsection{Remedy for the coverage problem}
Inspired by David Redish's idea, we decided to create previous and next time vectors which
give us information even when there is no spike.
This is the direction which seems promising at the moment since it tends to over come
the problem of coverage. what is the coverage problem? (see reference on tunning curves).
We then used the exponential kernel as our metric on the previous and next time data
to generate a distance metric which is the main input in Diffusion Maps algorithm.




